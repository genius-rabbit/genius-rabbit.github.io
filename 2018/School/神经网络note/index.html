<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="吾之所向 一往无前 愈挫愈勇 再接再厉"><link rel="stylesheet" type="text/css" href="//fonts.loli.net/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.4"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.4"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><link rel="stylesheet" type="text/css" href="https://unpkg.com/gitalk/dist/gitalk.css?v=2.0.4"><title>神经网络Note | Delta1037</title><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.2.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">神经网络Note</h1><a id="logo" href="/.">Delta1037</a><p class="description">技术是纯净的</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="search"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">神经网络Note</h1><div class="post-meta"><a href="/2018/School/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cnote/#comments" class="comment-count"></a><p><span class="date">Jul 30, 2018</span><span><a href="/categories/%E8%AF%BE%E7%A8%8B/" class="category">课程</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>Hits</i></i></span></p></div><div class="post-content"><h1 id="神经网络笔记"><a href="#神经网络笔记" class="headerlink" title="神经网络笔记"></a>神经网络笔记</h1><ul>
<li>代码链接<a target="_blank" rel="noopener" href="https://github.com/genius-rabbit/dianStudy/tree/master/naturalNetWork">github</a> :wink:</li>
</ul>
<h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><h5 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h5><pre><code>train_data = torchvision.datasets.CIFAR10(
    root=&#39;Datadir&#39;, train=True, transform=transforms.ToTensor(),download=True
)
</code></pre><ul>
<li>torchvision.datasets.CIFAR10:代表这是cifar10数据集,若为torchvision.datasets.MNIST则为mnist数据集</li>
<li>root:数据集的存放位置</li>
<li>train=True:代表这是训练集</li>
<li><p>download=True:如果数据不存在就会自行下载</p>
<p>train_loader = Data.DataLoader(dataset=train_data, batch_size=4, shuffle=True, num_workers=2)</p>
</li>
</ul>
<ul>
<li>dataset=train_data:代表从train_data加载数据</li>
<li>batch_size: 批训练的数据个数</li>
<li>shuffle: 是否要打乱数据(打乱比较好)</li>
<li>num_workers: 多线程读取数据</li>
</ul>
<h5 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h5><pre><code>optimizer = optim.SGD(net.parameters(), lr=0.03)
</code></pre><ul>
<li><p>定义优化器</p>
<ul>
<li>lr=0.03: 学习率为0.03</li>
<li>SGD: 使用SGD优化器</li>
</ul>
<p>loss_func = nn.CrossEntropyLoss()</p>
</li>
</ul>
<ul>
<li><p>定义损失函数</p>
<ul>
<li>CrossEntropyLoss:交叉熵损失</li>
</ul>
<h1 id="循环使用数据训练50次"><a href="#循环使用数据训练50次" class="headerlink" title="循环使用数据训练50次"></a>循环使用数据训练50次</h1><p>for epoch in range(50):</p>
<pre><code># 训练
# 每次训练之前的初始化loss&amp;acc
train_loss = 0
train_acc = 0
for i, data in enumerate(train_loader, 0):
    # 得到数据
    inputs, lables = data
    # 包装数据
    inputs, lables = Variable(inputs), Variable(lables)
    # 梯度清零
    optimizer.zero_grad()
    # 将输入输进网络&amp;forward
    out = net(inputs)
    # 计算损失
    loss = loss_func(out, lables)
    train_loss += loss.item()

    # 计算正确率
    pred = torch.max(out, 1)[1]
    train_acc += (pred == lables).sum().item()

    # backward&amp;optimize
    loss.backward()
    optimizer.step()

    train_loss += loss.data.item()

    if i % 2000 == 1999:
        # 格式化输出loss&amp;acc
        print(&#39;[%d,%5d] loss:%.6f&#39; % (epoch+1, i+1, train_loss/2000))
        print(&#39;[%d,%5d] acc: %.6f&#39; % (epoch + 1, i + 1, train_acc / (2000 * lables.size(0))))
        # 训练2000组数据打印数据之后的初始化
        train_acc = 0
        train_loss = 0
        torch.save(net, &#39;Alex0.03.pkl&#39;)
# 测试
# 正确率初始化
train_acc1 = 0
for i, data in enumerate(test_loader, 0):
    # 得到数据并包装数据
    inputs, lables = data
    inputs, lables = Variable(inputs), Variable(lables)
    # 计算正确率
    out = net(inputs)
    pred = torch.max(out, 1)[1]
    train_acc1 += (pred == lables).sum().item()
    # 阶段性输出
    if i % 1000 == 999:
        print(&#39;testDataAcc:[%d,%5d] acc: %.6f&#39; % (epoch + 1, i + 1, train_acc1 / (1000 * lables.size(0))))
        train_acc1 = 0
</code></pre></li>
</ul>
<h2 id="全连接神经网络"><a href="#全连接神经网络" class="headerlink" title="全连接神经网络"></a>全连接神经网络</h2><h3 id="note"><a href="#note" class="headerlink" title="note"></a>note</h3><ul>
<li>每层的节点数和层数可自己随意定义</li>
</ul>
<h3 id="参考代码"><a href="#参考代码" class="headerlink" title="参考代码"></a>参考代码</h3><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/genius-rabbit/dianStudy/blob/master/naturalNetWork/mnist.py">mnist全连接参考代码</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/genius-rabbit/dianStudy/blob/master/naturalNetWork/cifar10.py">cifar10全连接参考代码</a></li>
</ul>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><h3 id="参数计算"><a href="#参数计算" class="headerlink" title="参数计算"></a>参数计算</h3><ul>
<li>输入矩阵的格式:四个维度(样本数,图像高度,图像宽度,图像通道数)</li>
<li>卷积核(权重矩阵):四个维度(卷积核高,卷积核宽,输入通道数,输出通道数)<ul>
<li>卷积核输入的通道数由输入矩阵的通道数决定</li>
<li>输出矩阵的通道数由卷积核的输出通道数决定</li>
<li>输出矩阵的高和宽由输入矩阵,卷积核,扫描方式(padding,stride)所决定</li>
</ul>
</li>
<li>卷积之后的图像大小计算:outputSize = (inputSize + 2*padding - kernelSize)/stride + 1</li>
</ul>
<h3 id="LeNet-AlexNet-VGG-ResNet-GoogLeNet"><a href="#LeNet-AlexNet-VGG-ResNet-GoogLeNet" class="headerlink" title="LeNet AlexNet VGG ResNet GoogLeNet"></a>LeNet AlexNet VGG ResNet GoogLeNet</h3><h4 id="leNet"><a href="#leNet" class="headerlink" title="leNet"></a>leNet</h4><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/genius-rabbit/dianStudy/blob/master/naturalNetWork/leNet.py">cifar10数据集参考代码</a> <img src="https://www.geniusrabbit.top/wp-content/uploads/2018/08/LeNet.png" alt=""></li>
<li>输入是32x32的图像</li>
<li>C1是一个卷积层,卷积核为5x5,步长为1,没有填充,由(32-2*0-5)/1+1=28,得输出图像为28x28</li>
<li>S2是一个降采样层,kernelSize等于2,步长为2,没有填充,得到输出为14x14</li>
<li>C3是一个卷积层,卷积核大小为5x5,步长为1,没有填充,得输出图像为10x10</li>
<li>S4是一个降采样层,kernelSize等于2,步长为2,没有填充,得到输出为5x5</li>
<li>C5是一个卷积层,卷积核大小为5x5,步长为1,没有填充,得输出图像为1x1</li>
<li>F6是一个全连接层</li>
<li>输出有十个参数</li>
</ul>
<h4 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h4><p>-<a target="_blank" rel="noopener" href="https://github.com/genius-rabbit/dianStudy/blob/master/naturalNetWork/AlexNet.py">cifar10数据集参考代码</a> <strong>代码做了简化,使用单个运算核心运行的</strong> <img src="https://www.geniusrabbit.top/wp-content/uploads/2018/08/AlexNet.png" alt=""> note未跟随代码 - 卷积部分分为上下两块,卷积使用数据要看连接的虚线 - LRN层:对当前层的输出结果做平滑处理</p>
<h4 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h4><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/genius-rabbit/dianStudy/blob/master/naturalNetWork/VGG.py">cifar10数据集参考代码</a></li>
<li>连续conv较多,计算量巨大</li>
</ul>
<h4 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h4><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/genius-rabbit/dianStudy/blob/master/naturalNetWork/ResNet.py">cifar10数据集参考代码</a></li>
</ul>
<h6 id="深度网络的退化问题"><a href="#深度网络的退化问题" class="headerlink" title="深度网络的退化问题"></a>深度网络的退化问题</h6><pre><code>网络深度增加时,网络准确度出现饱和甚至开始下降
</code></pre><h6 id="使用残差学习解决退化问题"><a href="#使用残差学习解决退化问题" class="headerlink" title="使用残差学习解决退化问题"></a>使用残差学习解决退化问题</h6><p>残差学习相比原始特征直接学习更容易,当残差为0时,此时堆积层仅仅做了恒等映射.至少网络性能不会下降,实际上残差不会为0,也会使得堆积层在输入特征基础上学习到新的特征,从而有更好的性能 - 残差学习相当于一种短路机制</p>
<pre><code>残差学习单元
</code></pre><p><img src="https://www.geniusrabbit.top/wp-content/uploads/2018/08/ResNet.png" alt=""></p>
<pre><code>不同深度的ResNet网络
</code></pre><p><img src="https://www.geniusrabbit.top/wp-content/uploads/2018/08/ResNetAll.png" alt=""></p>
<h4 id="googLeNet"><a href="#googLeNet" class="headerlink" title="googLeNet"></a>googLeNet</h4><ul>
<li><a target="_blank" rel="noopener" href="http://noahsnail.com/2017/07/21/2017-7-21-GoogleNet%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/">论文翻译中文版</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/kuangliu/pytorch-cifar/blob/master/models/googlenet.py">cifar10数据集参考代码</a> GoogLeNet 与传统神经网络相比提出了Inception结构,用于增加神经网络的宽度和深度,Inception模型主要考虑多个不同的卷积核能够增强网络的适应能力,4个分支在最后通过一个聚合操作合并</li>
</ul>
<h6 id="Inception"><a href="#Inception" class="headerlink" title="Inception"></a>Inception</h6><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/hejin_some/article/details/78636586">参考博客</a></p>
<ul>
<li><p>v1 版本1: <img src="https://www.geniusrabbit.top/wp-content/uploads/2018/08/v10.png" alt=""> 随后文章指出这种 naive 结构存在着<strong>问题</strong>：每一层 Inception module 的 filters 参数量为所有分支上的总数和，多层 Inception 最终将导致 model 的参数数量庞大，对计算资源有更大的依赖 版本2: <img src="https://www.geniusrabbit.top/wp-content/uploads/2018/08/v11.png" alt=""> <strong>改进</strong>:Inception module with dimension reduction,在不损失模型特征表示能力的前提下，尽量减少 filters 的数量，达到降低模型复杂度的目的</p>
</li>
<li><p>v2 <img src="https://www.geniusrabbit.top/wp-content/uploads/2018/08/v2.jpeg" alt=""> 使用两个3*3的卷积核代替一个5*5的卷积核,在降低参数的同时,增加了更多的非线性变换</p>
</li>
<li><p>v3 <img src="https://www.geniusrabbit.top/wp-content/uploads/2018/08/v31.png" alt=""> <img src="https://www.geniusrabbit.top/wp-content/uploads/2018/08/v32.png" alt=""> <img src="https://www.geniusrabbit.top/wp-content/uploads/2018/08/v33.png" alt=""> 将一个3*3的卷积核拆解成3*1和1*3的卷积核 引入了 Factorization into small convolutions 的思想,将一个较大的二维卷积拆解为两个较小的一维卷积,加速了运算并减轻了过拟合</p>
</li>
<li><p>v4 <img src="https://www.geniusrabbit.top/wp-content/uploads/2018/08/v41.png" alt=""> <img src="https://www.geniusrabbit.top/wp-content/uploads/2018/08/v42.png" alt=""> <img src="https://www.geniusrabbit.top/wp-content/uploads/2018/08/v43.png" alt=""></p>
<p>V4 相比 V3 主要是结合了微软的 ResNet</p>
</li>
</ul>
</div><div class="post-copyright"><blockquote><p>Original author: delta1037</p><p>Original link: <a href="https://www.delta1037.cn/2018/School/神经网络note/">https://www.delta1037.cn/2018/School/神经网络note/</a></p><p>Copyright Notice: Please indicate the source of the reprint (must retain the author's signature and link)</p></blockquote></div><div class="tags"><a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a></div><div class="post-share"><div class="social-share"><span>Share:</span></div></div><div class="post-nav"><a href="/2018/Deploy/apache%E9%85%8D%E7%BD%AEssl/" class="pre">Apache配置ssl</a><a href="/2018/Deploy/owncloud-%E6%90%AD%E5%BB%BA/" class="next">owncloud 搭建</a></div><div id="comments"><div id="container"><script type="text/javascript" src="https://unpkg.com/gitalk/dist/gitalk.min.js?v=2.0.4"></script><script type="text/javascript" src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js?v=2.0.4"></script><script>var gitalk = new Gitalk({
  clientID: '79255b590bda0804dcc4',
  clientSecret: '6405c9b64ff65c24179669cb3839f0bd7053c151',
  repo: 'delta1037.github.io',
  owner: 'delta1037',
  admin: ['delta1037'],
  id: md5(window.location.pathname),
  distractionFreeMode: false,
  language: 'zh-CN',
  pagerDirection: 'last'
})
gitalk.render('container')</script></div></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">Contents</i></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AC%94%E8%AE%B0"><span class="toc-text">神经网络笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Note"><span class="toc-text">Note</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="toc-text">数据处理</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-text">训练模型</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E4%BD%BF%E7%94%A8%E6%95%B0%E6%8D%AE%E8%AE%AD%E7%BB%8350%E6%AC%A1"><span class="toc-text">循环使用数据训练50次</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">全连接神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#note"><span class="toc-text">note</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E4%BB%A3%E7%A0%81"><span class="toc-text">参考代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E8%AE%A1%E7%AE%97"><span class="toc-text">参数计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LeNet-AlexNet-VGG-ResNet-GoogLeNet"><span class="toc-text">LeNet AlexNet VGG ResNet GoogLeNet</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#leNet"><span class="toc-text">leNet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#AlexNet"><span class="toc-text">AlexNet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#VGG"><span class="toc-text">VGG</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ResNet"><span class="toc-text">ResNet</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E7%9A%84%E9%80%80%E5%8C%96%E9%97%AE%E9%A2%98"><span class="toc-text">深度网络的退化问题</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%AE%8B%E5%B7%AE%E5%AD%A6%E4%B9%A0%E8%A7%A3%E5%86%B3%E9%80%80%E5%8C%96%E9%97%AE%E9%A2%98"><span class="toc-text">使用残差学习解决退化问题</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#googLeNet"><span class="toc-text">googLeNet</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#Inception"><span class="toc-text">Inception</span></a></li></ol></li></ol></li></ol></li></ol></li></ol></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/Math/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/">布隆过滤器</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/Math/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/">最大熵模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/Economics/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E5%8D%81%E5%A4%A7%E5%8E%9F%E7%90%86/">经济学十大原理</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/Linux/Linux_fcntl/">fcntl</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/Linux/Linux_ioctl/">ioctl</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/Linux/Linux%E7%BB%88%E7%AB%AF%E5%8A%A8%E6%80%81%E5%88%B7%E6%96%B0/">Linux终端动态刷新</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/Linux/Linux%E8%AF%86%E5%88%AB%E6%8C%89%E9%94%AE/">Linux C语言实现按键即时识别</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/Linux/shell%E8%84%9A%E6%9C%AC%E5%9F%BA%E7%A1%80/">Shell脚本基础</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/Linux/Linux%E8%8E%B7%E5%8F%96%E6%97%B6%E9%97%B4/">Linux获取时间</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/C_C++/C++%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2/">C++ 类型转换</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/BugsFix/">BugsFix</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/C-C/">C/C++</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Dian%E5%9B%A2%E9%98%9F/">Dian团队</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Math/">Math</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%88%86%E5%B8%83%E5%BC%8F/">分布式</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%9A%E7%BA%BF%E7%A8%8B/">多线程</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%B7%A5%E7%A8%8B/">工程</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%97%B6%E5%85%89/">时光</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BB%8F%E6%B5%8E%E5%AD%A6/">经济学</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/">计算机网络</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%BE%E7%A8%8B/">课程</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%83%A8%E7%BD%B2/">部署</a><span class="category-list-count">13</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> Tags</i></div><div class="tagcloud"><a href="/tags/shadowsocks/" style="font-size: 15px;">shadowsocks</a> <a href="/tags/%E7%8A%B6%E6%80%81%E6%9C%BA/" style="font-size: 15px;">状态机</a> <a href="/tags/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA/" style="font-size: 15px;">算法导论</a> <a href="/tags/MySQL/" style="font-size: 15px;">MySQL</a> <a href="/tags/SSH/" style="font-size: 15px;">SSH</a> <a href="/tags/%E7%BC%96%E8%AF%91/" style="font-size: 15px;">编译</a> <a href="/tags/CodeStyle/" style="font-size: 15px;">CodeStyle</a> <a href="/tags/%E5%8D%8F%E8%AE%AE%E6%A0%88/" style="font-size: 15px;">协议栈</a> <a href="/tags/Socket/" style="font-size: 15px;">Socket</a> <a href="/tags/SQLite/" style="font-size: 15px;">SQLite</a> <a href="/tags/GitLab/" style="font-size: 15px;">GitLab</a> <a href="/tags/Oracle-Database-9i/" style="font-size: 15px;">Oracle Database 9i</a> <a href="/tags/%E5%AE%A1%E8%AE%A1/" style="font-size: 15px;">审计</a> <a href="/tags/SSL/" style="font-size: 15px;">SSL</a> <a href="/tags/%E4%BE%BF%E7%AD%BE/" style="font-size: 15px;">便签</a> <a href="/tags/Docker/" style="font-size: 15px;">Docker</a> <a href="/tags/GTK/" style="font-size: 15px;">GTK+</a> <a href="/tags/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/" style="font-size: 15px;">博客搭建</a> <a href="/tags/ownCloud/" style="font-size: 15px;">ownCloud</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">机器学习</a> <a href="/tags/Ubuntu/" style="font-size: 15px;">Ubuntu</a> <a href="/tags/%E7%BB%8F%E6%B5%8E%E5%AD%A6/" style="font-size: 15px;">经济学</a> <a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/Linux%E5%B0%8F%E7%B3%BB%E7%BB%9F/" style="font-size: 15px;">Linux小系统</a> <a href="/tags/Ping/" style="font-size: 15px;">Ping</a> <a href="/tags/Shell/" style="font-size: 15px;">Shell</a> <a href="/tags/Math/" style="font-size: 15px;">Math</a> <a href="/tags/kubernetes/" style="font-size: 15px;">kubernetes</a> <a href="/tags/MPI/" style="font-size: 15px;">MPI</a> <a href="/tags/OpenMP/" style="font-size: 15px;">OpenMP</a> <a href="/tags/posix/" style="font-size: 15px;">posix</a> <a href="/tags/Git/" style="font-size: 15px;">Git</a> <a href="/tags/NS-3/" style="font-size: 15px;">NS-3</a> <a href="/tags/%E6%95%8F%E6%8D%B7%E5%BC%80%E5%8F%91/" style="font-size: 15px;">敏捷开发</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">神经网络</a> <a href="/tags/%E6%B5%8B%E8%AF%95/" style="font-size: 15px;">测试</a> <a href="/tags/SMS/" style="font-size: 15px;">SMS</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> Archive</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-you"> Blogroll</i></div><ul></ul><a href="http://dian.org.cn/" title="华中科技大学Dian团队" target="_blank">华中科技大学Dian团队</a></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">Site Map</a> |  <a href="/atom.xml">Subscribe to this site</a> |  <a href="/about/">Contact the blogger</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次，本站总访客数:<i id="busuanzi_container_site_uv"><i id="busuanzi_value_site_uv"></i></i>人</p><p><span> Copyright &copy;<a href="/." rel="nofollow">delta1037.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a target="_blank" rel="noopener" href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.4"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.4" async></script><link rel="stylesheet" type="text/css" href="/share/css/share.css"><script type="text/javascript" src="/share/js/social-share.js" charset="utf-8"></script><script type="text/javascript" src="/share/js/qrcode.js" charset="utf-8"></script></body></html>